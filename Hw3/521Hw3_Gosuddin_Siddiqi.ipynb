{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 Factor Analysis\n",
    "**Due Date: 11/14/2017**\n",
    "\n",
    "Name: Gosuddin Siddiqi<br>\n",
    "Student Number: 1627383"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Convex low rank matrix factorization\n",
    "Consider the problem,\n",
    "$$\\min_X \\frac{1}{2}\\|R - X\\|_F^2 + \\lambda\\|X\\|_*$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the sythetic data\n",
    "srand(123);\n",
    "m = 500;\n",
    "n = 200;\n",
    "R = randn(m,n);\n",
    "λ = 30.0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prox_nuclear_norm (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define functions\n",
    "# objective function\n",
    "function cvx_mf_obj(X)\n",
    "    # TODO: complete the objective function\n",
    "    U,Σ,v = svd(X)\n",
    "    return (0.5*(vecnorm(R-X)^2) + λ*sum(Σ))\n",
    "end\n",
    "\n",
    "# gradient function\n",
    "function cvx_mf_smooth_grad(X)\n",
    "    # TODO: return gradient\n",
    "    return(X-R)\n",
    "end\n",
    "\n",
    "# prox of ‖⋅‖_*\n",
    "function prox_nuclear_norm(Z, α)\n",
    "    # TODO: return the prox solution\n",
    "    U,Σ,V = svd(Z)\n",
    "    prox_v = zeros(Σ)\n",
    "    for i = 1:length(Σ)\n",
    "        if Σ[i] > α\n",
    "            prox_v[i] = Σ[i] - α\n",
    "        end\n",
    "    end\n",
    "    return (U*diagm(prox_v))*transpose(V)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    1, obj 5.00e+04, err 3.09e+02\n",
      "iter    2, obj 5.00e+04, err 2.65e-13\n"
     ]
    }
   ],
   "source": [
    "# proximal gradient method\n",
    "η   = 1.0;\n",
    "tol = 1e-6;\n",
    "itm = 1000;\n",
    "err = Inf;\n",
    "\n",
    "X   = copy(R);\n",
    "X⁻  = copy(X);\n",
    "for iter = 1:itm\n",
    "    # TODO: Implement proximal gradient descent method\n",
    "    \n",
    "    X⁻ = copy(X)\n",
    "    ∇ = cvx_mf_smooth_grad(X)\n",
    "    X = prox_nuclear_norm( (X - η .* ∇), λ.*η)\n",
    "    \n",
    "    \n",
    "    obj = cvx_mf_obj(X)\n",
    "    err = vecnorm(X-X⁻) / η\n",
    "    \n",
    "    @printf(\"iter %4d, obj %1.2e, err %1.2e\\n\", iter, obj, err);\n",
    "    err < tol && break;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis the result\n",
    "k = rank(X)\n",
    "U,S,V = svd(R);\n",
    "X_k = U[:,1:k]*diagm(S[1:k])*(V[:,1:k]).';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Nonconvex low rank matrix factorization\n",
    "Consider problem,\n",
    "$$\\min_{B,F} \\frac{1}{2}\\|R - BF\\|_F^2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "noncvx_mf_grad_F (generic function with 1 method)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define functions\n",
    "# objective\n",
    "function noncvx_mf_obj(B,F)\n",
    "    # TODO: return objective funciton\n",
    "    return 0.5*(vecnorm(R - (B*F)) ^2)\n",
    "end\n",
    "\n",
    "# gradient\n",
    "function noncvx_mf_grad_B(B,F)\n",
    "    # TODO: return gradient function w.r.t. B\n",
    "    return ((B*F - R)*transpose(F))\n",
    "end\n",
    "\n",
    "function noncvx_mf_grad_F(B,F)\n",
    "    # TODO: return gradient function w.r.t. F\n",
    "    return (transpose(B)*(B*F - R))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  1000, obj 3.47e+04, err 2.66e-02\n",
      "iter  2000, obj 3.47e+04, err 9.55e-03\n",
      "iter  3000, obj 3.47e+04, err 4.81e-03\n",
      "iter  4000, obj 3.47e+04, err 2.66e-03\n",
      "iter  5000, obj 3.47e+04, err 1.53e-03\n",
      "iter  6000, obj 3.47e+04, err 9.01e-04\n",
      "iter  7000, obj 3.47e+04, err 5.36e-04\n",
      "iter  8000, obj 3.47e+04, err 3.22e-04\n",
      "iter  9000, obj 3.47e+04, err 1.94e-04\n",
      "iter 10000, obj 3.47e+04, err 1.18e-04\n",
      "iter 11000, obj 3.47e+04, err 7.23e-05\n",
      "iter 12000, obj 3.47e+04, err 4.44e-05\n",
      "iter 13000, obj 3.47e+04, err 2.74e-05\n",
      "iter 14000, obj 3.47e+04, err 1.70e-05\n",
      "iter 15000, obj 3.47e+04, err 1.06e-05\n",
      "iter 16000, obj 3.47e+04, err 6.71e-06\n",
      "iter 17000, obj 3.47e+04, err 4.28e-06\n",
      "iter 18000, obj 3.47e+04, err 2.76e-06\n",
      "iter 19000, obj 3.47e+04, err 1.80e-06\n",
      "iter 20000, obj 3.47e+04, err 1.19e-06\n"
     ]
    }
   ],
   "source": [
    "# PALM algorithm\n",
    "tol = 1e-6;\n",
    "itm = 30000;\n",
    "err = Inf;\n",
    "\n",
    "B   = randn(m,k);\n",
    "B⁻  = copy(B);\n",
    "F   = randn(k,n);\n",
    "F⁻  = copy(F);\n",
    "\n",
    "g_B = noncvx_mf_grad_B(B,F)\n",
    "g_F = noncvx_mf_grad_F(B,F)\n",
    "\n",
    "for iter = 1:itm\n",
    "    # update B\n",
    "    # TODO: what is step size for B should be?\n",
    "    #       implement 1 gradient step\n",
    "    η_B = 0.0\n",
    "    β_B = vecnorm(F⁻*transpose(F⁻))\n",
    "    η_B = 1.0/β_B\n",
    "    B = B⁻ - (η_B*g_B)\n",
    "    \n",
    "    # update F\n",
    "    # TODO: what is step size for F should be?\n",
    "    #       implement 1 gradient step\n",
    "    η_F = 0.0\n",
    "    β_F = vecnorm(transpose(B)*B)\n",
    "    η_F = 1.0/β_F\n",
    "    F = F⁻ - (η_F * g_F)\n",
    "    \n",
    "    \n",
    "    # update convergent history\n",
    "    err = vecnorm(B - B⁻) + vecnorm(F - F⁻);\n",
    "    g_B = noncvx_mf_grad_B(B,F)\n",
    "    g_F = noncvx_mf_grad_F(B,F)\n",
    "    obj = noncvx_mf_obj(B, F);\n",
    "\n",
    "    copy!(B⁻, B);\n",
    "    copy!(F⁻, F);\n",
    "    iter % 1000 == 0 && @printf(\"iter %5d, obj %1.2e, err %1.2e\\n\", iter, obj, err);\n",
    "    err < tol && break;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vecnorm(X_k - B * F) / vecnorm(X_k) = 3.6788657894983514e-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.6788657894983514e-5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare result\n",
    "@show vecnorm(X_k - B*F)/vecnorm(X_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value looks almost the same but only after 30000 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Robust Risk Decomposition\n",
    "Consider problem,\n",
    "$$\\min_{B,F} \\rho_\\kappa(\\bar{R} - BF), \\quad\\text{s.t. }B^TB = I.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(R̄) = (499,251)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "N   = 499;\n",
    "T   = 251;\n",
    "fid = open(\"Data/returns.bin\", \"r\");\n",
    "R   = read(fid, Float64, (N,T));\n",
    "close(fid);\n",
    "R̄ = broadcast(-, R, mean(R,2));\n",
    "@show size(R̄)\n",
    "κ = 1.0;\n",
    "itm = 8000;\n",
    "tol = 1e-6;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prox_svd (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: implement PALM to solve the objective\n",
    "#       you could follow the steps we are using so far\n",
    "#       * define functions\n",
    "\n",
    "function huber(r)\n",
    "    h = 0.0\n",
    "    for I in eachindex(r)\n",
    "        h = 0.0\n",
    "        if abs(r[I]) > κ\n",
    "            h += (abs(r[I])*κ) - (0.5* (κ^2));\n",
    "        else\n",
    "            h += 0.5 * (abs(r[I])^2);\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "function huber_∇(x)\n",
    "    for I in eachindex(x)\n",
    "        if x[I]< -κ\n",
    "            x[I] = -κ\n",
    "        elseif x[I] > κ\n",
    "            x[I] = κ\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "\n",
    "end\n",
    "\n",
    "function robust(B,F)\n",
    "    return (huber(R̄ - (B*F)))\n",
    "end\n",
    "\n",
    "\n",
    "function grad_B(B,F)\n",
    "    r = R̄ - (B * F)\n",
    "    h_d = huber_∇(r)\n",
    "    derv_B = - (h_d * transpose(F))\n",
    "    return derv_B\n",
    "end\n",
    "    \n",
    "function grad_F(B,F)\n",
    "    r = R̄ - (B * F)\n",
    "    h_d = huber_∇(r)\n",
    "    derv_F = - (transpose(B)*h_d)\n",
    "    return derv_F\n",
    "end\n",
    "    \n",
    "function prox_svd(Z)\n",
    "    U, Σ, V = svd(Z)\n",
    "    return (U * transpose(V))\n",
    "\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  1000, obj 4.81e-02, err 2.48e+00\n",
      "iter  2000, obj 4.59e-02, err 3.53e-02\n",
      "iter  3000, obj 2.49e-02, err 2.54e-02\n",
      "iter  4000, obj 1.97e-02, err 2.06e-02\n",
      "iter  5000, obj 1.82e-02, err 1.57e-02\n",
      "iter  6000, obj 1.94e-02, err 1.27e-02\n",
      "iter  7000, obj 2.09e-02, err 1.27e-02\n",
      "iter  8000, obj 2.18e-02, err 9.77e-03\n",
      "iter  9000, obj 2.24e-02, err 8.93e-03\n",
      "iter 10000, obj 2.25e-02, err 9.77e-03\n",
      "iter 11000, obj 2.25e-02, err 1.20e-02\n",
      "iter 12000, obj 2.24e-02, err 1.40e-02\n",
      "iter 13000, obj 2.21e-02, err 1.38e-02\n",
      "iter 14000, obj 2.14e-02, err 1.34e-02\n",
      "iter 15000, obj 2.10e-02, err 1.92e-02\n",
      "iter 16000, obj 2.17e-02, err 1.17e-02\n",
      "iter 17000, obj 2.17e-02, err 8.31e-03\n",
      "iter 18000, obj 2.11e-02, err 7.95e-03\n",
      "iter 19000, obj 2.06e-02, err 6.11e-03\n",
      "iter 20000, obj 2.03e-02, err 4.76e-03\n",
      "iter 21000, obj 2.04e-02, err 3.97e-03\n",
      "iter 22000, obj 2.06e-02, err 3.80e-03\n",
      "iter 23000, obj 2.09e-02, err 3.87e-03\n",
      "iter 24000, obj 2.11e-02, err 3.91e-03\n",
      "iter 25000, obj 2.15e-02, err 3.17e-03\n",
      "iter 26000, obj 2.19e-02, err 2.62e-03\n",
      "iter 27000, obj 2.24e-02, err 2.45e-03\n",
      "iter 28000, obj 2.29e-02, err 2.03e-03\n",
      "iter 29000, obj 2.33e-02, err 1.80e-03\n",
      "iter 30000, obj 2.38e-02, err 1.67e-03\n"
     ]
    }
   ],
   "source": [
    "#       * implement solver\n",
    "tol = 1e-6;\n",
    "itm = 30000;\n",
    "err = Inf;\n",
    "\n",
    "B   = randn(N,k);\n",
    "B⁻  = copy(B);\n",
    "F   = randn(k,T);\n",
    "F⁻  = copy(F);\n",
    "\n",
    "rd_∇_B = grad_B(B,F)\n",
    "rd_∇_F = grad_F(B,F)\n",
    "\n",
    "\n",
    "for iter = 1:itm\n",
    "    # update B\n",
    "    # TODO: what is step size for B should be?\n",
    "    #       implement 1 gradient step\n",
    "    η_B = 0.0\n",
    "    β_B = (vecnorm(F⁻))^2\n",
    "    η_B = 1.0/β_B\n",
    "    B = prox_svd(B⁻ - (η_B*rd_∇_B))\n",
    "    \n",
    "    # update F\n",
    "    # TODO: what is step size for F should be?\n",
    "    #       implement 1 gradient step\n",
    "    η_F = 1.0\n",
    "    F = F⁻ - (η_F * rd_∇_F)\n",
    "    \n",
    "    \n",
    "    # update convergent history\n",
    "    err = vecnorm(B - B⁻) + vecnorm(F - F⁻);\n",
    "    rd_∇_B = grad_B(B,F)\n",
    "    rd_∇_F = grad_F(B,F)\n",
    "    obj = robust(B, F);\n",
    "\n",
    "    copy!(B⁻, B);\n",
    "    copy!(F⁻, F);\n",
    "    iter % 1000 == 0 && @printf(\"iter %5d, obj %1.2e, err %1.2e\\n\", iter, obj, err);\n",
    "    err < tol && break;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind = 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the portfolio\n",
    "b1  = B[:,1];\n",
    "ind = indmax(abs(b1));\n",
    "@show ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
